# 顺理成章的MySQL时期

作为一款在线流量分析产品，对其功能的要求自然是分析流量了。早期的 `Yandex.Metrica` 以提供固定报表的形式帮助用户进行分析，例如分析访问者使用的设备、访问者的来源分布之类。其实这也是早期分析类产品的典型特征之一，分析纬度和场景固定的，新的分析需求往往需要 `IT` 人员参与。

从技术角度来看，当时还处于关系统数据库称霸的时期，所以 `Yandex` 在内部其他产品中使用了 `MySQL` 数据库作为统计信息系统的底层存储软件。 `Yandex.Metrica` 的第一版架构顺理成章延续了这套内部稳定成熟的 `MySQL` 方案，并将其作为它的数据库存储和分析引擎的解决方案。

因为 `Yandex` 内部的这套 `MySQL` 方案使用了 `MyISAM` 表引擎，所以 `Yandex.Metrica` 也延续了表引擎的选择。这类分析场景更关注数据写入和查询的性能，不关系事务操作（ `MyISAM` 表引擎**不支持事务特性**）。相比 `InnoDB` 表引擎， `MyISAM` 表引擎在分析场景中具有更好的性能。

业内一个常识性的认知是：**按顺序存储的数据会拥有更高的查询性能**。因为读取顺序文件会用更少的**磁盘寻道**和**旋转延迟**时间（这里主要指**机械硬盘**），同时顺序读取也能利用操作系统层面文件缓存的**预读**功能，所以数据库的查询性能与数据在物理磁盘上的存储顺序息息相关。然而这套 `MySQL` 方案无法做到顺序存储。

`MyISAM` 表引擎使用 `B+` 树结构存储索引，而数据则使用另外单独的存储文件（ `InnoDB` 表引擎使用 `B+` 树同时存储索引和数据，数据直接挂载在叶子节点中）。如果只考虑单线程的写入场景，并且在写入过程中不涉及数据删除或者更新操作，那么数据会依次按照写入的顺序被写入文件并落至磁盘。然而现实的场景不可能如此简单。

流量的数据采集链路是这样的：网站端的应用程序首先通过 `Yandex` 提供的站点 `SDK` 实时采集数据并发送到远端的接收系统，再由接收系统将数据写入 `MySQL` 集群。整个过程都是实时进行的，并且数据接收系统是一个分布式系统，所以他们会并行、随机将数据写入 `MySQL` 集群。这最终导致了数据在磁盘中是完全随机存储的，并且会产生大量的磁盘碎片。

市面上一块典型的 `7200` 转的 `SATA` 磁盘的 `IOPS` （**每秒能处理的请求数**）仅为 `100` 左右，也就是说每秒只能执行 `100` 次随机读取。假设一次随机读取返回 `10` 行数据，那么查询 `100,000` 行记录则需要至少 `100` 秒，这种相应时间显然是不可接受的。

> [!tip|label: 提示]
> `RAID` 可以提供磁盘 `IOPS` 性能，但并不能解决根本问题。 `SSD` 随机读取性能很高，但是考虑到硬件成本和集群规模，不可能全部采用 `SSD` 存储。

随着时间的推移， `MySQL` 中的数据越来越多（截止 `2011` 年，存储的数据超过 `5800` 亿行）。虽然 `Yandex` 又额外做了许多优化，成功将 `90%` 的分析报告控制在 `26` 秒内返回，但是这套技术方案越来越显得力不从心。
